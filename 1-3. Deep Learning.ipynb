{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2521c58b-be60-4756-8be4-3c746e5553a0",
   "metadata": {},
   "source": [
    "## Info\n",
    "\n",
    "* Author : hyg4438(HongYongGi)/email : hyg4438@gachon.ac.kr\n",
    "\n",
    "## log\n",
    "* written date : 20220221\n",
    "\n",
    "# Python Deep Learning PyTorch\n",
    "## Part 3. Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c5a74-5fc3-4748-8c29-d31be0a9461d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 01 딥러닝의 정의\n",
    "\n",
    "* 딥러닝은 새로운 모델의 개념이 아니라 신경망이 발전한 모델이다. \n",
    "    * 신경망은 학습하는 알고리즘의 특성상 과적합이 심하게 일어나며 Gradient Vanishing이 일어남(해결하기 위해서 학습 과정 내에서 과적합을 방지할 수 있는 SVM과 Ensemble Learning이 많이 쓰임)\n",
    "    \n",
    "    \n",
    "* 딥러닝의 정의는 크게 두 가지로 나눌 수 있다.\n",
    "    * 딥러닝은 2개 이상의 Hidden Layer를 지니고 있는 다층 신경망(Deep Neural Network, DNN)이라 할 수 있습니다.\n",
    "    * 신경망을 기반으로 한 모델이기 때문이다.\n",
    "    * 딥러닝이 본격적으로 발전하게 된 것은 단순히 모델이 깊어졌기 때문이 아니다. 딥러닝에는 Graphical Representation Learning이라는 특징이 있기 때문이다.\n",
    "    \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cee390-85ae-46dc-a9ae-720415c39f4b",
   "metadata": {},
   "source": [
    "## 02 딥러닝이 발전하게 된 계기\n",
    "\n",
    "* 딥러닝이 발전하게 된 데에는 크게 두 가지 원인이 있음\n",
    "\n",
    "    1. 신경망의 단점으로 지적돼왔던 과적합과 Gradient Vanishing을 완화시킬 수 있는 알고리즘이 발전한 것 \n",
    "    2. 타 알고리즘 대비 학습 시간이 매우 오래 걸리는 문제가 있었는데 Graphics Processing Unit(GPU)을 신경망의 연산에 사용할 수 있게 되면서 이를 해결하게 된 것이다.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25acd9-bc06-41c7-acb4-c2b93b9a29ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03 딥러닝의 발전을 이끈 알고리즘\n",
    "\n",
    "### 03.1 Dropout\n",
    "\n",
    "* Dropout은 신경망의 학습 과정 중 Layer의 노드를 랜덤하게 Drop함으로써 Generalization 효과를 가져오게 하는 테크닉\n",
    "\n",
    "* Dropout을 적용한다는 것은 Weight Matrix에 랜덤하게 일부 Column에 0을 집어넣어 연산을 한다고 이해하면 됨\n",
    "\n",
    "* Dropout을 적용할 때는 얼마나 랜덤하게 Dropout한 것인지에 대한 확률 값을 지정해야 하며 이는 Input layer와 Hidden Layer에도 적용할 수 있다.\n",
    "\n",
    "* 또한 Epoch마다 랜덤하게 Dropout한다.\n",
    "\n",
    "ex) Input Layer에 대해 20%. Hidden Layer에 대해 30%를 적용한다고 가정하면, 그러면 Input Data의 Column에 랜덤하게 20%는 0을 집어넣고 Weight Matrix의 Column에도 랜덤하게 30%는 0을 집어넣어 Feed Forward를 진행하고 Back Propagation을 진행한다. 그다음 Epoch에서 Dropout을 적용할 때 이번 Epoch와 독립적으로 랜덤하게 적용됨.\n",
    "(즉, 이전 Epoch과는 다른 Column에 0을 집어넣는 것이다. 이러한 방식으로 계속 연산하면 과적합을 어느 정도 방지하는 효과를 가져온다는 것이다. Dropout은 처음 제안된 이후로도 현재까지 기본적으로 신경망을 디자인할 때 범용적으로 많이 사용되고 있는 테크닉이다.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64dcf001-6a85-46e4-9a43-bd74771323e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1. Module Import '''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027c6455-a4a2-4276-9a0e-c0389558d646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.10.2  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "''' 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5e96f1b-c422-41c7-8f82-7df481773d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adf3f453-9d44-4404-bf24-23d232aed992",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 3. MNIST 데이터 다운로드 (Train set, Test set 분리하기) '''\n",
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                               train = True,\n",
    "                               download = True,\n",
    "                               transform = transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3ae9fa4-21ab-4d96-ab80-1f394a06cb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "''' 4. 데이터 확인하기 (1) '''\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60529419-9850-4659-b7df-41285fc0846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' 5. 데이터 확인하기 (2) '''\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "    plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4d2af78-49c9-4c4f-af11-03f1ba4b7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbf43458-d54d-47ab-8ed2-259413db42b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "201e7d40-8657-4236-9582-ed7db3730ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20c3baa3-1f4a-4330-9428-8282ff887cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9. 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7d572e8-ef44-4975-8ce4-8da7d02961ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.459884\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.322014\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 2.304762\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 2.293983\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 2.335092\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 2.323176\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 2.278047\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 2.287803\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 2.233943\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 2.178774\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 2.2425, \tTest Accuracy: 28.86 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 2.247509\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 2.172461\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 2.196803\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 2.146855\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 2.062236\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 1.931362\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 1.931328\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 1.728715\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 1.468849\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 1.325496\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 1.2809, \tTest Accuracy: 60.40 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 1.410939\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 1.401603\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 1.167657\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 1.024785\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.943048\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.532590\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.810866\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.728561\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.615814\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.829595\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.7118, \tTest Accuracy: 79.58 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.616832\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.908554\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.794386\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.763509\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.739354\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.498680\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.657111\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.426486\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.771951\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.571523\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.5187, \tTest Accuracy: 85.35 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.719963\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.519640\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.526967\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.455397\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.569277\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.569720\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.694408\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.516203\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.594856\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.363334\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.4373, \tTest Accuracy: 87.43 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.393111\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.480729\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.585498\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.264173\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.570264\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.281131\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.492568\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.455624\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.331643\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.468666\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.3946, \tTest Accuracy: 88.45 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.293745\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.376228\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.282996\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.268506\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.492834\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.501122\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.276566\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.331227\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.563020\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.247727\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.3776, \tTest Accuracy: 89.00 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.121894\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.433189\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.382248\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.643878\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.496416\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.335329\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.320511\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.260337\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.430723\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.406407\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.3557, \tTest Accuracy: 89.63 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.380872\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.620240\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.341590\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.254120\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.416059\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.130712\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.518472\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.163336\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.859944\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.526870\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.3438, \tTest Accuracy: 90.06 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.197904\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.200219\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.243906\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.419893\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.368374\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.168470\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.273753\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.291905\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.498958\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.391151\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.3361, \tTest Accuracy: 90.28 % \n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tTrain Loss: 0.306476\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tTrain Loss: 0.276604\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tTrain Loss: 0.210344\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tTrain Loss: 0.273150\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tTrain Loss: 0.574663\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tTrain Loss: 0.376605\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tTrain Loss: 0.556391\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tTrain Loss: 0.484664\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tTrain Loss: 0.173542\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tTrain Loss: 0.267886\n",
      "\n",
      "[EPOCH: 11], \tTest Loss: 0.3256, \tTest Accuracy: 90.45 % \n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tTrain Loss: 0.419677\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tTrain Loss: 0.412549\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tTrain Loss: 0.311110\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tTrain Loss: 0.252685\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tTrain Loss: 0.300331\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tTrain Loss: 0.208634\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tTrain Loss: 0.383124\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tTrain Loss: 0.379945\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tTrain Loss: 0.280357\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tTrain Loss: 0.392319\n",
      "\n",
      "[EPOCH: 12], \tTest Loss: 0.3156, \tTest Accuracy: 91.02 % \n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tTrain Loss: 0.269176\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tTrain Loss: 0.181410\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tTrain Loss: 0.576619\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tTrain Loss: 0.238276\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tTrain Loss: 0.141398\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tTrain Loss: 0.281198\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tTrain Loss: 0.100978\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tTrain Loss: 0.264725\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tTrain Loss: 0.387667\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tTrain Loss: 0.271761\n",
      "\n",
      "[EPOCH: 13], \tTest Loss: 0.3107, \tTest Accuracy: 91.05 % \n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tTrain Loss: 0.347408\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tTrain Loss: 0.153005\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tTrain Loss: 0.469508\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tTrain Loss: 0.287155\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tTrain Loss: 0.490712\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tTrain Loss: 0.103769\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tTrain Loss: 0.221814\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tTrain Loss: 0.762934\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tTrain Loss: 0.376724\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tTrain Loss: 0.385989\n",
      "\n",
      "[EPOCH: 14], \tTest Loss: 0.3026, \tTest Accuracy: 91.27 % \n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tTrain Loss: 0.516243\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tTrain Loss: 0.233335\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tTrain Loss: 0.322095\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tTrain Loss: 0.556715\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tTrain Loss: 0.351470\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tTrain Loss: 0.191714\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tTrain Loss: 0.073381\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tTrain Loss: 0.605309\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tTrain Loss: 0.462389\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tTrain Loss: 0.873132\n",
      "\n",
      "[EPOCH: 15], \tTest Loss: 0.2995, \tTest Accuracy: 91.38 % \n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tTrain Loss: 0.289606\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tTrain Loss: 0.279078\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tTrain Loss: 0.266766\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tTrain Loss: 0.314568\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tTrain Loss: 0.201126\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tTrain Loss: 0.311630\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tTrain Loss: 0.573525\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tTrain Loss: 0.288151\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tTrain Loss: 0.292612\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tTrain Loss: 0.231266\n",
      "\n",
      "[EPOCH: 16], \tTest Loss: 0.2897, \tTest Accuracy: 91.79 % \n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tTrain Loss: 0.428965\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tTrain Loss: 0.216345\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tTrain Loss: 0.125999\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tTrain Loss: 0.295688\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tTrain Loss: 0.199793\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tTrain Loss: 0.731296\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tTrain Loss: 0.448690\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tTrain Loss: 0.304899\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tTrain Loss: 0.436995\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tTrain Loss: 0.648427\n",
      "\n",
      "[EPOCH: 17], \tTest Loss: 0.2860, \tTest Accuracy: 91.85 % \n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tTrain Loss: 0.268182\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tTrain Loss: 0.250277\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tTrain Loss: 0.194867\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tTrain Loss: 0.254130\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tTrain Loss: 0.146019\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tTrain Loss: 0.345005\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tTrain Loss: 0.230447\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tTrain Loss: 0.183742\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tTrain Loss: 0.187509\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tTrain Loss: 0.216051\n",
      "\n",
      "[EPOCH: 18], \tTest Loss: 0.2790, \tTest Accuracy: 91.99 % \n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tTrain Loss: 0.125679\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tTrain Loss: 0.114995\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tTrain Loss: 0.259835\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tTrain Loss: 0.139524\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tTrain Loss: 0.094151\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tTrain Loss: 0.196129\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tTrain Loss: 0.340017\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tTrain Loss: 0.138609\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tTrain Loss: 0.117108\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tTrain Loss: 0.195301\n",
      "\n",
      "[EPOCH: 19], \tTest Loss: 0.2811, \tTest Accuracy: 92.00 % \n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tTrain Loss: 0.442182\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tTrain Loss: 0.349007\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tTrain Loss: 0.345640\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tTrain Loss: 0.111167\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tTrain Loss: 0.286100\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tTrain Loss: 0.119911\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tTrain Loss: 0.117865\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tTrain Loss: 0.511594\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tTrain Loss: 0.374100\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tTrain Loss: 0.337390\n",
      "\n",
      "[EPOCH: 20], \tTest Loss: 0.2696, \tTest Accuracy: 92.00 % \n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tTrain Loss: 0.114023\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tTrain Loss: 0.469892\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tTrain Loss: 0.180521\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tTrain Loss: 0.305694\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tTrain Loss: 0.244654\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tTrain Loss: 0.392818\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tTrain Loss: 0.192074\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tTrain Loss: 0.432813\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tTrain Loss: 0.367075\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tTrain Loss: 0.188638\n",
      "\n",
      "[EPOCH: 21], \tTest Loss: 0.2657, \tTest Accuracy: 92.32 % \n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tTrain Loss: 0.190200\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tTrain Loss: 0.094532\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tTrain Loss: 0.183646\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tTrain Loss: 0.195768\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tTrain Loss: 0.277174\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tTrain Loss: 0.480219\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tTrain Loss: 0.440333\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tTrain Loss: 0.278895\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tTrain Loss: 0.174109\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tTrain Loss: 0.315459\n",
      "\n",
      "[EPOCH: 22], \tTest Loss: 0.2646, \tTest Accuracy: 92.35 % \n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tTrain Loss: 0.264187\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tTrain Loss: 0.212728\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tTrain Loss: 0.129906\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tTrain Loss: 0.158864\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tTrain Loss: 0.593539\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tTrain Loss: 0.174849\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tTrain Loss: 0.364605\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tTrain Loss: 0.250118\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tTrain Loss: 0.201204\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tTrain Loss: 0.283301\n",
      "\n",
      "[EPOCH: 23], \tTest Loss: 0.2539, \tTest Accuracy: 92.71 % \n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tTrain Loss: 0.137838\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tTrain Loss: 0.094079\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tTrain Loss: 0.094066\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tTrain Loss: 0.352969\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tTrain Loss: 0.326238\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tTrain Loss: 0.073920\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tTrain Loss: 0.232926\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tTrain Loss: 0.437682\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tTrain Loss: 0.278790\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tTrain Loss: 0.140505\n",
      "\n",
      "[EPOCH: 24], \tTest Loss: 0.2509, \tTest Accuracy: 92.87 % \n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tTrain Loss: 0.092717\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tTrain Loss: 0.140817\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tTrain Loss: 0.127875\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tTrain Loss: 0.407231\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tTrain Loss: 0.535290\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tTrain Loss: 0.277227\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tTrain Loss: 0.174987\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tTrain Loss: 0.669032\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tTrain Loss: 0.452108\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tTrain Loss: 0.247296\n",
      "\n",
      "[EPOCH: 25], \tTest Loss: 0.2455, \tTest Accuracy: 92.86 % \n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tTrain Loss: 0.589164\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tTrain Loss: 0.454433\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tTrain Loss: 0.158750\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tTrain Loss: 0.125800\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tTrain Loss: 0.188781\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tTrain Loss: 0.245619\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tTrain Loss: 0.214035\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tTrain Loss: 0.142955\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tTrain Loss: 0.222759\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tTrain Loss: 0.232923\n",
      "\n",
      "[EPOCH: 26], \tTest Loss: 0.2415, \tTest Accuracy: 93.10 % \n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tTrain Loss: 0.188983\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tTrain Loss: 0.473676\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tTrain Loss: 0.105078\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tTrain Loss: 0.334063\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tTrain Loss: 0.067856\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tTrain Loss: 0.252700\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tTrain Loss: 0.206859\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tTrain Loss: 0.187172\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tTrain Loss: 0.065544\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tTrain Loss: 0.514226\n",
      "\n",
      "[EPOCH: 27], \tTest Loss: 0.2426, \tTest Accuracy: 93.17 % \n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tTrain Loss: 0.577503\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tTrain Loss: 0.361834\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tTrain Loss: 0.185850\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tTrain Loss: 0.279777\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tTrain Loss: 0.101685\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tTrain Loss: 0.177638\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tTrain Loss: 0.218091\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tTrain Loss: 0.122060\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tTrain Loss: 0.195643\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tTrain Loss: 0.292275\n",
      "\n",
      "[EPOCH: 28], \tTest Loss: 0.2329, \tTest Accuracy: 93.38 % \n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tTrain Loss: 0.266083\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tTrain Loss: 0.433729\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tTrain Loss: 0.292690\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tTrain Loss: 0.189161\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tTrain Loss: 0.079890\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tTrain Loss: 0.251613\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tTrain Loss: 0.546938\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tTrain Loss: 0.165324\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tTrain Loss: 0.519497\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tTrain Loss: 0.091963\n",
      "\n",
      "[EPOCH: 29], \tTest Loss: 0.2338, \tTest Accuracy: 93.09 % \n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tTrain Loss: 0.347310\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tTrain Loss: 0.184201\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tTrain Loss: 0.143216\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tTrain Loss: 0.075193\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tTrain Loss: 0.127333\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tTrain Loss: 0.158348\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tTrain Loss: 0.308332\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tTrain Loss: 0.081250\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tTrain Loss: 0.035656\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tTrain Loss: 0.169908\n",
      "\n",
      "[EPOCH: 30], \tTest Loss: 0.2264, \tTest Accuracy: 93.45 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da2c98-18f8-4614-845b-25145f3d84dc",
   "metadata": {},
   "source": [
    "93.45%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd8b6e-42be-429a-b239-8c2edfa9a138",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dropout 사용 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69b4a2c0-353e-45a0-b039-206026cfb9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(28 * 28, 512)\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 28 * 28)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.sigmoid(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = F.sigmoid(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = F.log_softmax(x, dim = 1)\n",
    "#         return x\n",
    "\n",
    "\n",
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5 # 추가\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob) # 추가\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob) # 추가\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e3ec5bd-be8c-4096-8fae-d270c5c286fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b831a18-e650-4225-a7a6-5e980dc2a3f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.515180\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.397249\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 2.310801\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 2.295760\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 2.245982\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 2.293839\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 2.309143\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 2.275986\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 2.325297\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 2.340597\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 2.2808, \tTest Accuracy: 16.60 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 2.253518\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 2.248695\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 2.314086\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 2.236936\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 2.317499\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 2.260069\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 2.233555\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 2.249979\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 2.215293\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 2.044239\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 2.0771, \tTest Accuracy: 34.03 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 2.095742\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 2.077420\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 2.052608\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 1.883779\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 1.830164\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 1.645703\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 1.510822\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 1.355227\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 1.412150\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 1.383880\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.1821, \tTest Accuracy: 60.74 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 1.363607\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 1.122789\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 1.202098\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.961972\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 1.328577\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 1.260158\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 1.240056\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 1.524523\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.957368\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 1.081737\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.8730, \tTest Accuracy: 71.75 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.989387\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 1.066932\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.934855\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.675888\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.671273\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 1.042224\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.895538\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 1.052307\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.566260\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 1.092879\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.7471, \tTest Accuracy: 76.15 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.702773\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.949305\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.855808\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.916275\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.679147\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.698870\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.713978\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.905757\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.538984\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.747656\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.6355, \tTest Accuracy: 81.11 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.686671\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.577513\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 1.007648\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 1.046632\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.596184\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.499352\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.470752\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.775329\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.898967\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.591312\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.5463, \tTest Accuracy: 84.16 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.740223\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.944492\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.848100\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.481422\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.973025\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.820263\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.470388\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.623481\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.373670\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.833273\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.4887, \tTest Accuracy: 85.44 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.457758\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.827809\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.545189\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.674158\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 1.035232\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.562972\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.876593\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.394986\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.460582\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.731908\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.4527, \tTest Accuracy: 86.32 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.353382\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.821720\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.434967\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.408865\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.750486\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.669382\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.748038\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.519105\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.700589\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.541200\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.4255, \tTest Accuracy: 87.43 % \n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tTrain Loss: 0.385706\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tTrain Loss: 0.471293\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tTrain Loss: 0.312216\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tTrain Loss: 0.525808\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tTrain Loss: 0.419771\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tTrain Loss: 0.416670\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tTrain Loss: 0.578765\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tTrain Loss: 0.358256\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tTrain Loss: 0.578354\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tTrain Loss: 0.150251\n",
      "\n",
      "[EPOCH: 11], \tTest Loss: 0.4054, \tTest Accuracy: 87.94 % \n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tTrain Loss: 0.293464\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tTrain Loss: 0.404573\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tTrain Loss: 0.663154\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tTrain Loss: 0.506656\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tTrain Loss: 0.776142\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tTrain Loss: 0.692556\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tTrain Loss: 0.384436\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tTrain Loss: 0.260499\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tTrain Loss: 0.502984\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tTrain Loss: 0.481761\n",
      "\n",
      "[EPOCH: 12], \tTest Loss: 0.3903, \tTest Accuracy: 88.43 % \n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tTrain Loss: 0.782365\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tTrain Loss: 0.400877\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tTrain Loss: 0.358775\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tTrain Loss: 0.338750\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tTrain Loss: 0.314751\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tTrain Loss: 0.396136\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tTrain Loss: 0.414710\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tTrain Loss: 0.244675\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tTrain Loss: 0.756169\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tTrain Loss: 0.367765\n",
      "\n",
      "[EPOCH: 13], \tTest Loss: 0.3750, \tTest Accuracy: 88.84 % \n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tTrain Loss: 0.338232\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tTrain Loss: 0.304229\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tTrain Loss: 0.404917\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tTrain Loss: 0.468403\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tTrain Loss: 0.259891\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tTrain Loss: 0.519122\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tTrain Loss: 0.732641\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tTrain Loss: 0.306057\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tTrain Loss: 0.494884\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tTrain Loss: 0.370374\n",
      "\n",
      "[EPOCH: 14], \tTest Loss: 0.3626, \tTest Accuracy: 89.18 % \n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tTrain Loss: 0.436349\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tTrain Loss: 0.704871\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tTrain Loss: 0.372755\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tTrain Loss: 0.456916\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tTrain Loss: 0.477979\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tTrain Loss: 0.251302\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tTrain Loss: 0.455887\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tTrain Loss: 0.579299\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tTrain Loss: 0.414310\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tTrain Loss: 0.569165\n",
      "\n",
      "[EPOCH: 15], \tTest Loss: 0.3527, \tTest Accuracy: 89.52 % \n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tTrain Loss: 0.321765\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tTrain Loss: 0.336601\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tTrain Loss: 0.335648\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tTrain Loss: 0.374260\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tTrain Loss: 0.576244\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tTrain Loss: 0.684554\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tTrain Loss: 0.168308\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tTrain Loss: 0.205184\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tTrain Loss: 0.802929\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tTrain Loss: 0.734243\n",
      "\n",
      "[EPOCH: 16], \tTest Loss: 0.3429, \tTest Accuracy: 89.74 % \n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tTrain Loss: 0.379769\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tTrain Loss: 0.435679\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tTrain Loss: 0.349096\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tTrain Loss: 0.399478\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tTrain Loss: 0.280451\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tTrain Loss: 0.495663\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tTrain Loss: 0.536971\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tTrain Loss: 0.272213\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tTrain Loss: 0.323539\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tTrain Loss: 0.349551\n",
      "\n",
      "[EPOCH: 17], \tTest Loss: 0.3343, \tTest Accuracy: 90.09 % \n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tTrain Loss: 0.716442\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tTrain Loss: 0.391391\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tTrain Loss: 0.216781\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tTrain Loss: 0.543455\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tTrain Loss: 0.455287\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tTrain Loss: 0.388618\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tTrain Loss: 0.385241\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tTrain Loss: 0.491770\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tTrain Loss: 0.366201\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tTrain Loss: 0.416346\n",
      "\n",
      "[EPOCH: 18], \tTest Loss: 0.3272, \tTest Accuracy: 90.30 % \n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tTrain Loss: 0.734713\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tTrain Loss: 0.447853\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tTrain Loss: 0.409582\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tTrain Loss: 0.266677\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tTrain Loss: 0.388295\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tTrain Loss: 0.171312\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tTrain Loss: 0.697465\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tTrain Loss: 0.534746\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tTrain Loss: 0.477136\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tTrain Loss: 0.493515\n",
      "\n",
      "[EPOCH: 19], \tTest Loss: 0.3186, \tTest Accuracy: 90.55 % \n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tTrain Loss: 0.701486\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tTrain Loss: 0.248267\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tTrain Loss: 0.655981\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tTrain Loss: 0.786725\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tTrain Loss: 0.454419\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tTrain Loss: 0.243206\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tTrain Loss: 0.449571\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tTrain Loss: 0.380461\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tTrain Loss: 0.457603\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tTrain Loss: 0.283672\n",
      "\n",
      "[EPOCH: 20], \tTest Loss: 0.3125, \tTest Accuracy: 90.76 % \n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tTrain Loss: 0.545543\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tTrain Loss: 0.555238\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tTrain Loss: 1.065583\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tTrain Loss: 0.293160\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tTrain Loss: 0.404814\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tTrain Loss: 0.655372\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tTrain Loss: 0.204909\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tTrain Loss: 0.163985\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tTrain Loss: 0.981343\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tTrain Loss: 0.738400\n",
      "\n",
      "[EPOCH: 21], \tTest Loss: 0.3063, \tTest Accuracy: 90.79 % \n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tTrain Loss: 0.700606\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tTrain Loss: 0.278346\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tTrain Loss: 0.198155\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tTrain Loss: 0.163444\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tTrain Loss: 0.283962\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tTrain Loss: 0.391757\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tTrain Loss: 0.351576\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tTrain Loss: 0.374677\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tTrain Loss: 0.180546\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tTrain Loss: 0.743610\n",
      "\n",
      "[EPOCH: 22], \tTest Loss: 0.3016, \tTest Accuracy: 90.95 % \n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tTrain Loss: 0.558332\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tTrain Loss: 0.550414\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tTrain Loss: 0.803105\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tTrain Loss: 0.498592\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tTrain Loss: 0.188952\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tTrain Loss: 0.282732\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tTrain Loss: 0.568735\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tTrain Loss: 0.410005\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tTrain Loss: 0.541771\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tTrain Loss: 0.561801\n",
      "\n",
      "[EPOCH: 23], \tTest Loss: 0.2958, \tTest Accuracy: 91.10 % \n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tTrain Loss: 0.386800\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tTrain Loss: 0.176625\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tTrain Loss: 0.348932\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tTrain Loss: 0.149755\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tTrain Loss: 0.191002\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tTrain Loss: 0.174344\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tTrain Loss: 0.621617\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tTrain Loss: 0.203974\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tTrain Loss: 0.306368\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tTrain Loss: 0.305443\n",
      "\n",
      "[EPOCH: 24], \tTest Loss: 0.2918, \tTest Accuracy: 91.23 % \n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tTrain Loss: 0.285565\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tTrain Loss: 0.288718\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tTrain Loss: 0.346626\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tTrain Loss: 0.211400\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tTrain Loss: 0.194131\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tTrain Loss: 0.314792\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tTrain Loss: 0.257062\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tTrain Loss: 0.475232\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tTrain Loss: 0.160468\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tTrain Loss: 0.749387\n",
      "\n",
      "[EPOCH: 25], \tTest Loss: 0.2845, \tTest Accuracy: 91.45 % \n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tTrain Loss: 0.560178\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tTrain Loss: 0.252657\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tTrain Loss: 0.135209\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tTrain Loss: 0.203654\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tTrain Loss: 0.231281\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tTrain Loss: 0.239150\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tTrain Loss: 0.440110\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tTrain Loss: 0.243371\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tTrain Loss: 0.184721\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tTrain Loss: 0.254366\n",
      "\n",
      "[EPOCH: 26], \tTest Loss: 0.2798, \tTest Accuracy: 91.59 % \n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tTrain Loss: 0.500284\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tTrain Loss: 0.308529\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tTrain Loss: 0.290186\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tTrain Loss: 0.511267\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tTrain Loss: 0.186392\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tTrain Loss: 0.384352\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tTrain Loss: 0.331415\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tTrain Loss: 0.320802\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tTrain Loss: 0.260911\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tTrain Loss: 0.162665\n",
      "\n",
      "[EPOCH: 27], \tTest Loss: 0.2771, \tTest Accuracy: 91.66 % \n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tTrain Loss: 0.659071\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tTrain Loss: 0.541407\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tTrain Loss: 0.327172\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tTrain Loss: 0.144003\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tTrain Loss: 0.227701\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tTrain Loss: 0.182126\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tTrain Loss: 0.469280\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tTrain Loss: 0.330732\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tTrain Loss: 0.532912\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tTrain Loss: 0.504528\n",
      "\n",
      "[EPOCH: 28], \tTest Loss: 0.2719, \tTest Accuracy: 91.90 % \n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tTrain Loss: 0.264953\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tTrain Loss: 0.171507\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tTrain Loss: 0.333462\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tTrain Loss: 0.527134\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tTrain Loss: 0.474019\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tTrain Loss: 0.688460\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tTrain Loss: 0.530712\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tTrain Loss: 0.360073\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tTrain Loss: 0.413552\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tTrain Loss: 0.121870\n",
      "\n",
      "[EPOCH: 29], \tTest Loss: 0.2688, \tTest Accuracy: 92.06 % \n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tTrain Loss: 0.416063\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tTrain Loss: 0.177380\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tTrain Loss: 0.274888\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tTrain Loss: 0.703295\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tTrain Loss: 0.468510\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tTrain Loss: 0.162809\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tTrain Loss: 0.211529\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tTrain Loss: 0.315355\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tTrain Loss: 0.439468\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tTrain Loss: 0.283808\n",
      "\n",
      "[EPOCH: 30], \tTest Loss: 0.2651, \tTest Accuracy: 92.04 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04ce76-82e6-4323-b58c-912402be97af",
   "metadata": {},
   "source": [
    "92.04%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da299f2-df2c-4970-97d2-45629f3a4ccd",
   "metadata": {},
   "source": [
    "## Activation 함수\n",
    "* ReLU 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "589a37b5-bbcf-47a5-a3c4-b7cdf23f82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f27ca790-9442-4690-8156-ce9b144ebe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040850ca-9d53-4afc-a522-87834f60cf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.295533\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 1.933292\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 1.033345\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.828259\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.928539\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.529546\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.242869\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.484704\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.422585\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.518841\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.3201, \tTest Accuracy: 90.94 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.348783\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.424429\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.235638\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.314581\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0acf9-4045-4907-83d6-5e594970eb09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889d289-508b-433f-b792-2e99dcb643c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df5245-69dc-46e3-85c7-eaccb2d208c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c8e80-1ffe-4d21-b87e-dd420c9ef387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba7866-ffe5-4c9e-a6f4-dcec091b2c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4462a63-8229-4113-b72b-a15bdaafcb61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ff35b-916e-4c5f-a5b3-8c26aa886336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9c49f-a707-4c55-8126-8ea99dd46f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
